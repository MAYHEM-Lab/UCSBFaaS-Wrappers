git pull
export PREFIX=/Users/ckrintz/RESEARCH/lambda/

#update the following with your aws account details
#role with admin rights
export ACCT=443592014519
export ROLE=adminlambda
export AWSRole=arn:aws:iam::${ACCT}:role/${ROLE}
export AWSPROFILE=cjk1
export REG=us-west-2

export SPOTBKTWEST=cjktestbkt
export SPOTBKTEAST=cjktestbkteast
export BDBENCH=cjk-gammaray-bdbenchmark
export APITESTBKT=cjk-apitest-bucket
export APITESTTABLE=cjk-apitest-bucket
export APIOUTBKT=cjk-spotwraptest0831
export SPOTTABLE=spotFns
export GAMMATABLE=gammaRays

#if you haven't done so already, clone the big-data-benchmark to the west 
aws --profile cjk1 s3 mb s3://${BDBENCH}
aws --profile cjk1 s3 sync s3://big-data-benchmark s3://${BDBENCH)

#if you haven't done so already, make the buckets and table
aws --profile cjk1 s3 mb s3://${SPOTBKTWEST}
aws --profile cjk1 s3 mb s3://${SPOTBKTEAST}
aws --profile cjk1 s3 mb s3://${APIOUTBKT}
aws --profile cjk1 s3 mb s3://${APITESTBKT}
# make tables if you have not already in DynamoDB
aws --profile cjk1 dynamodb create-table --region ${REG} --table-name ${APITESTTABLE} --attribute-definitions AttributeName=id,AttributeType=N --key-schema AttributeName=id,KeyType=HASH --provisioned-throughput ReadCapacityUnits=5,WriteCapacityUnits=5
aws --profile cjk1 dynamodb create-table --region ${REG} --table-name ${SPOTTABLE} --attribute-definitions AttributeName=reqID,AttributeType=S --key-schema AttributeName=reqID,KeyType=HASH --provisioned-throughput ReadCapacityUnits=10,WriteCapacityUnits=20
aws --profile cjk1 dynamodb create-table --region ${REG} --table-name ${GAMMATABLE} --attribute-definitions AttributeName=reqID,AttributeType=S --key-schema AttributeName=reqID,KeyType=HASH --provisioned-throughput ReadCapacityUnits=10,WriteCapacityUnits=20

#go and add a stream to both your GAMMATABLE and SPOTTABLE so that we can download the data in order
#keep old and new data in the stream; note the date/time component (after .../stream/) and store in these vars for use later:
SPOT_STREAMID=2017-09-03T22:09:22.445
GR_STREAMID=2017-09-01T21:10:57.071

#1 per job type for imageProc app
aws --profile cjk1 s3 mb s3://image-proc-c
aws --profile cjk1 s3 mb s3://image-proc-t
aws --profile cjk1 s3 mb s3://image-proc-s
aws --profile cjk1 s3 mb s3://image-proc-d
aws --profile cjk1 s3 mb s3://image-proc-f
#1 per job type for map reduce app (mine use old names: ns=C and gr=D now, empty=S)
aws --profile cjk1 s3 mb s3://spot-mr-bkt
aws --profile cjk1 s3 mb s3://spot-mr-bkt-f
aws --profile cjk1 s3 mb s3://spot-mr-bkt-gr
aws --profile cjk1 s3 mb s3://spot-mr-bkt-ns
aws --profile cjk1 s3 mb s3://spot-mr-bkt-d

cd ${PREFIX}/UCSBFaaS-Wrappers/
deactivate
#this dir is hardcoded in makeConfigs.py, so change it there if you move it
mkdir boto144 
cd boto144
rm -rf venv
virtualenv venv --python=python3
pip install 'boto3==1.4.4' --force-reinstall
cd venv/lib/python3.6/site-packages/botocore
patch -b < cd ${PREFIX}/UCSBFaaS-Wrappers/gammaRay/client144.patch
deactivate

cd ${PREFIX}/UCSBFaaS-Wrappers/gammaRay/
deactivate
rm -rf venv
virtualenv venv --python=python3
source venv/bin/activate
pip install fleece
cd venv/lib/python3.6/site-packages/fleece
patch -b < ../../../../../xray.patch


cd venv/lib/python3.6/site-packages/botocore
patch -b < ../../../../../client.patch
cd ${PREFIX}/UCSBFaaS-Wrappers/gammaRay/
cd ${PREFIX}/UCSBFaaS-Wrappers/gammaRay/

#setup the microbenchmarks
cd ${PREFIX}/UCSBFaaS-Wrappers/lambda-python/apis
#do this once (modify the file to set S3BKT=$APITESTBKT and DYNAMODB=$APITESTTABLE)
chmod +x prepare.sh
#modify this file to go from 1 to 1000 instead of 1 to 100
./prepare.sh

#if you want to clean everything out of AWS and start fresh see/run CLEANUP below
cd ${PREFIX}/tools/timings

#delete all of the lambdas by name, in AWS
#modify restConfigsWest.json and restConfigsEast.json to add the names of the other lambdas you want to delete
#note that this deletes everything!  any names that aren't found are just skipped (missing lambda names are not a problem)
./cleanupLambdas.sh ${AWSPROFILE} ${AWSROLE}
#check the AWS Lambda management console 
#edit restConfigs[West,East].json to add others

#generate the config files for the lambdas for all cases (CASES list is below)
cd ${PREFIX}/UCSBFaaS-Wrappers/gammaRay/
rm -rf configs
mkdir -p configs
#edit makeConfigs to update triggerBuckets and triggerTables datastructures to have your bucket names
#triggerBuckets for reducerCoordinator must match those in overhead??.sh files and runs below
python makeConfigs.py configs --swbkt ${SPOTBKTWEST} --swbkteast ${SPOTBKTEAST}

#three each: make, West, East for
###nothing / clean (C)
###fleece only (tracing) (T)
###fleece only (tracing+daemon) (F)
###original spotwrap (static insertion of wrapper) (S)
###gammaray (dynamic insertion of wrapper) (D)
python setupApps.py -f configs/configC.json -p ${AWSPROFILE} --deleteAll
python setupApps.py -f configs/configC.json -p ${AWSPROFILE} --no_spotwrap 
python setupApps.py -f configs/configEastC.json -p ${AWSPROFILE} --no_spotwrap 
python setupApps.py -f configs/configT.json -p ${AWSPROFILE} --deleteAll
python setupApps.py -f configs/configT.json -p ${AWSPROFILE} --no_spotwrap --turn_on_tracing
python setupApps.py -f configs/configEastT.json -p ${AWSPROFILE} --no_spotwrap --turn_on_tracing
python setupApps.py -f configs/configF.json -p ${AWSPROFILE} --deleteAll
python setupApps.py -f configs/configF.json -p ${AWSPROFILE} --no_spotwrap --turn_on_tracing --with_fleece
python setupApps.py -f configs/configEastF.json -p ${AWSPROFILE} --no_spotwrap --turn_on_tracing --with_fleece
python setupApps.py -f configs/configS.json -p ${AWSPROFILE} --deleteAll
python setupApps.py -f configs/configS.json -p ${AWSPROFILE} --spotFnsTableName spotFns --spotFnsTableRegion ${REG}
python setupApps.py -f configs/configEastS.json -p ${AWSPROFILE} --spotFnsTableName spotFns --spotFnsTableRegion ${REG}
python setupApps.py -f configs/configD.json -p ${AWSPROFILE} --deleteAll
python setupApps.py -f configs/configD.json -p ${AWSPROFILE} --no_spotwrap --spotFnsTableName gammaRays --spotFnsTableRegion ${REG} --gammaRay
python setupApps.py -f configs/configEastD.json -p ${AWSPROFILE} --no_spotwrap --spotFnsTableName gammaRays --spotFnsTableRegion ${REG} --gammaRay

cd ../tools/timings
#clean out the databases gammaRays and spotFns, as well as the logs
./cleanupAWS.sh ${AWSPROFILE}
#run cleanupDB.sh repeatedly until it says 'Table 'tableName' is empty.'
./cleanupDB.sh ${AWSPROFILE}

#only if you want to, blow away the timings directories from past runs, update END to the last directory ID
export END=100
for i in $(seq 1 $END); do rm -rf ${PREFIX}/UCSBFaaS-Wrappers/gammaRay/apps/${i}; done
for i in $(seq 1 $END); do rm -rf ${PREFIX}/UCSBFaaS-Wrappers/gammaRay/apps/map-reduce/${i}; done
for i in $(seq 1 $END); do rm -rf ${PREFIX}/UCSBFaaS-Wrappers/tools/dynamodb/${i}; done
for i in $(seq 1 $END); do rm -rf ${PREFIX}/UCSBFaaS-Wrappers/tools/cloudwatch/${i}; done
for i in $(seq 1 $END); do rm -rf ${PREFIX}/UCSBFaaS-Wrappers/tools/cloudwatch/logs/; done

#download the streams, you will use these to diff out the old stuff when you get new stuff
#update these commands with your stream name
python ../get_stream_data.py arn:aws:dynamodb:${REG}:${ACCT}:table/${SPOTTABLE}/stream/${SPOT_STREAMID} -p ${AWSPROFILE} >& streamS.base
python ../get_stream_data.py arn:aws:dynamodb:${REG}:${ACCT}:table/${GAMMATABLE}/stream/${GR_STREAMID} -p ${AWSPROFILE} >&  streamD.base

#run synchronized map reduce job overhead measurement (second param is count)
export COUNT=50
nohup ./overheadC.sh ${AWSPROFILE} ${COUNT} ${BDBENCH} &
#wait between each at least 12 mins to ensure completion to avoid contention
nohup ./overheadF.sh ${AWSPROFILE} ${COUNT} ${BDBENCH} &
#wait between each at least 12 mins to ensure completion to avoid contention
nohup ./overheadT.sh ${AWSPROFILE} ${COUNT} ${BDBENCH} &
#wait between each at least 12 mins to ensure completion to avoid contention
nohup ./overheadS.sh ${AWSPROFILE} ${COUNT} ${BDBENCH} &
#wait between each at least 12 mins to ensure completion to avoid contention
nohup ./overheadD.sh ${AWSPROFILE} ${COUNT} ${BDBENCH} &

#run microbenchmarks (default is 100 times) in gammaRay/apis/ directory
#modify each of the files replace the bucket with $APITESTBKT and the tablenames with $APITESTTABLE
cd ${PREFIX}/tools/timings
#todo: setup triggers

#imageProc app, diff out the differences between the streams and the base and store them in base for next program
./ohead_imgProc.sh ${AWSPROFILE}
python ../get_stream_data.py arn:aws:dynamodb:${REG}:${ACCT}:table/${SPOTTABLE}/stream/${SPOT_STREAMID} -p ${AWSPROFILE} >& streamS.new
diff -b -B streamS.base streamS.new| awk -F"> " '{print $2}' > imageProcS.stream
cat imageProcS.stream >> streamS.base
python ../get_stream_data.py arn:aws:dynamodb:${REG}:${ACCT}:table/${GAMMATABLE}/stream/${GR_STREAMID} -p ${AWSPROFILE} >&  streamD.base
diff -b -B streamD.base streamD.new| awk -F"> " '{print $2}' > imageProcD.stream
cat imageProcD.stream >> streamD.base

#microbenchmarks, no streams needed for micro benchmarks
./micro.sh ${AWSPROFILE}

#map-reduce 
./mr.sh ${AWSPROFILE} ${ACCT} {COUNT}

#webapp
./webapp.sh ${AWSPROFILE} ${ACCT} {COUNT}

#dbsync, capture streams for this one
./dbsync.sh ${AWSPROFILE} ${ACCT} {COUNT}
python ../get_stream_data.py arn:aws:dynamodb:${REG}:${ACCT}:table/${SPOTTABLE}/stream/${SPOT_STREAMID} -p ${AWSPROFILE} >& streamS.new
diff -b -B streamS.base streamS.new| awk -F"> " '{print $2}' > dbsyncS.stream
cat dbsyncS.stream >> streamS.base
python ../get_stream_data.py arn:aws:dynamodb:${REG}:${ACCT}:table/${GAMMATABLE}/stream/${GR_STREAMID} -p ${AWSPROFILE} >&  streamD.base
diff -b -B streamD.base streamD.new| awk -F"> " '{print $2}' > dbsyncD.stream
cat dbsyncD.stream >> streamD.base

#process streams to generate pdfs for S (D is the same)
python ../ddb_parser.py dbsyncS.stream
python ../ddb_parser.py imageProcS.stream

#######  CLEANUP ############
#delete everything in AWS, and I mean everything! 
cd ${PREFIX}/tools/timings
#add any other lambdas you want to delete to restConfigsWest.json and restConfigsEast.json
#depending on where your lambdas are
./cleanupAWS.sh ${AWSPROFILE}
#run cleanupDB.sh repeatedly until it says that the table is empty! this can take awhile
./cleanupDB.sh ${AWSPROFILE}
#delete all of the lambdas by name, in AWS (edit restConfigs[West,East].json to add others)
./cleanupLambdas.sh ${AWSPROFILE} ${AWSROLE}
#you now need to go and run makeConfigs.sh and all of the setupApps.py above
export END=100
for i in $(seq 1 $END); do rm -rf ${PREFIX}/UCSBFaaS-Wrappers/gammaRay/apps/${i}; done
for i in $(seq 1 $END); do rm -rf ${PREFIX}/UCSBFaaS-Wrappers/gammaRay/apps/map-reduce/${i}; done
for i in $(seq 1 $END); do rm -rf ${PREFIX}/UCSBFaaS-Wrappers/tools/dynamodb/${i}; done
for i in $(seq 1 $END); do rm -rf ${PREFIX}/UCSBFaaS-Wrappers/tools/cloudwatch/${i}; done
for i in $(seq 1 $END); do rm -rf ${PREFIX}/UCSBFaaS-Wrappers/tools/cloudwatch/logs/; done

#download the streams, you will use these to diff out the old stuff when you get new stuff
#update these commands with your stream name
python ../get_stream_data.py arn:aws:dynamodb:${REG}:${ACCT}:table/${SPOTTABLE}/stream/${SPOT_STREAMID} -p ${AWSPROFILE} >& streamS.base
python ../get_stream_data.py arn:aws:dynamodb:${REG}:${ACCT}:table/${GAMMATABLE}/stream/${GR_STREAMID} -p ${AWSPROFILE} >&  streamD.base
#now you can start fresh
======================
CASES:
C - nothing/clean
T - tracing
F - tracing + fleece daemon
S - static spotwrap (original)
D - dynamic spotwrap (gammaray)
