git pull
export PREFIX=/Users/ckrintz/RESEARCH/lambda/UCSBFaaS-Wrappers

#update the following with your aws account details
#role with admin rights
export ACCT=443592014519
export ROLE=adminlambda
export AWSRole=arn:aws:iam::${ACCT}:role/${ROLE}
export AWSPROFILE=cjk1
export REG=us-west-2
export FLEECEDIR=${PREFIX}/fleece0.13.0
export FLEECE=${FLEECEDIR}/venv/lib/python3.6/site-packages/fleece
export BOTOCOREDIR=${PREFIX}/boto144
export BOTOCORE=${BOTOCOREDIR}/venv/lib/python3.6/site-packages/botocore
export LOCALLIBDIR=${PREFIX}/gammaRay
export LOCALLIBS=${LOCALLIBDIR}/venv/lib/python3.6/site-packages

export SPOTBKTWEST=cjktestbkt
export SPOTBKTEAST=cjktestbkteast
export BDBENCH=cjk-gammaray-bdbenchmark
export APITESTBKT=cjk-apitest-bucket
export APITESTTABLE=noTriggerTestTable
export SPOTTABLE=spotFns
export GAMMATABLE=gammaRays

#if you haven't done so already, clone the big-data-benchmark to the west 
aws --profile ${AWSPROFILE} s3 mb s3://${BDBENCH}
aws --profile ${AWSPROFILE} s3 sync s3://big-data-benchmark s3://${BDBENCH)

#if you haven't done so already, make the buckets and table
aws --profile ${AWSPROFILE} s3 mb s3://${SPOTBKTWEST}
aws --profile ${AWSPROFILE} s3 mb s3://${SPOTBKTEAST}
aws --profile ${AWSPROFILE} s3 mb s3://${APITESTBKT}
# make tables if you have not already in DynamoDB
aws --profile ${AWSPROFILE} dynamodb create-table --region ${REG} --table-name ${APITESTTABLE} --attribute-definitions AttributeName=id,AttributeType=N --key-schema AttributeName=id,KeyType=HASH --provisioned-throughput ReadCapacityUnits=5,WriteCapacityUnits=5
aws --profile ${AWSPROFILE} dynamodb create-table --region ${REG} --table-name ${SPOTTABLE} --attribute-definitions AttributeName=reqID,AttributeType=S --key-schema AttributeName=reqID,KeyType=HASH --provisioned-throughput ReadCapacityUnits=10,WriteCapacityUnits=20
aws --profile ${AWSPROFILE} dynamodb create-table --region ${REG} --table-name ${GAMMATABLE} --attribute-definitions AttributeName=reqID,AttributeType=S --key-schema AttributeName=reqID,KeyType=HASH --provisioned-throughput ReadCapacityUnits=10,WriteCapacityUnits=20

#make the subscriptions for sns topics to trigger S3ModPy (webapp)
cd ${PREFIX}/tools
./make_subs.sh ${AWSPROFILE} ${ACCT} ${REG} 
#to delete them (if ever you need to, go to the SNS Management Console, click Subscriptions and Topics, select and use Action->Delete


#go and add a stream to both your GAMMATABLE and SPOTTABLE so that we can download the data in order
#keep old and new data in the stream; note the date/time component (after .../stream/) and store in these vars for use later:
SPOT_STREAMID=2017-09-03T22:09:22.445
GR_STREAMID=2017-09-04T21:33:11.568

#1 per job type for imageProc app
aws --profile ${AWSPROFILE} s3 mb s3://image-proc-c
aws --profile ${AWSPROFILE} s3 mb s3://image-proc-t
aws --profile ${AWSPROFILE} s3 mb s3://image-proc-s
aws --profile ${AWSPROFILE} s3 mb s3://image-proc-d
aws --profile ${AWSPROFILE} s3 mb s3://image-proc-f
#1 per job type for map reduce app (mine use old names: ns=C and gr=D now, empty=S)
aws --profile ${AWSPROFILE} s3 mb s3://spot-mr-bkt
aws --profile ${AWSPROFILE} s3 mb s3://spot-mr-bkt-f
aws --profile ${AWSPROFILE} s3 mb s3://spot-mr-bkt-gr
aws --profile ${AWSPROFILE} s3 mb s3://spot-mr-bkt-ns
aws --profile ${AWSPROFILE} s3 mb s3://spot-mr-bkt-d

deactivate
#$BOTOCORE is passed as first argument to makeConfigs.py
mkdir -p ${BOTOCOREDIR}
cd ${BOTOCOREDIR}
virtualenv venv --python=python3
source venv/bin/activate
pip install --upgrade pip setuptools wheel
pip install 'boto3==1.4.4' --force-reinstall
cd ${BOTOCORE}
patch -b < ${PREFIX}/gammaRay/client144.patch
deactivate

#$FLEECE is passed as second argument to makeConfigs.py
mkdir -p ${FLEECEDIR}
cd ${FLEECEDIR}
virtualenv venv --python=python3
source venv/bin/activate
pip install --upgrade pip setuptools wheel
pip install 'fleece==0.13.0' --force-reinstall
cd ${FLEECE}
patch -b < ${PREFIX}/gammaRay/xray0130.patch
deactivate

#$LOCALLIBS is passed as third argument to makeConfigs.py
cd ${LOCALLIBDIR}  #this should be gammaRays directory for most 
virtualenv venv --python=python3
#setup a venv for local testing
source venv/bin/activate
pip install 'fleece==0.13.0' --force-reinstall #fleece installs boto3 the version will work fine for local testing

#setup the microbenchmarks
cd ${PREFIX}/gammaRay/micro-benchmarks
#do this ONLY once to fill the table with data (modify the file to set S3BKT=$APITESTBKT and DYNAMODB=$APITESTTABLE)
./prepare.sh ${APITESTBKT} ${APITESTTABLE}

cd ${PREFIX}/tools/timings
#clean up all logs and spotFns and gammaRays database tables, if any
./cleanupAWS.sh ${AWSPROFILE} ${PREFIX}
#this is just the DB cleanup part in the above script (to speed things up), 
#run it repeatedly until it says 'Table 'tableName' is empty.'
./cleanupDB.sh ${AWSPROFILE} ${PREFIX}

#delete all of the lambdas by name, in AWS
#modify restConfigsWest.json and restConfigsEast.json to add the names of the other lambdas you want to delete
#note that this deletes everything!  any names that aren't found are just skipped (missing lambda names are not a problem)
./cleanupLambdas.sh ${AWSPROFILE} ${AWSROLE} ${PREFIX}
#check the AWS Lambda management console 
#edit restConfigs[West,East].json to add others

#generate the config files for the lambdas for all cases (CASES list is below)
cd ${PREFIX}/gammaRay
rm -rf configs
mkdir -p configs
#edit makeConfigs to update triggerBuckets and triggerTables datastructures to have your bucket names
#triggerBuckets for reducerCoordinator must match those in overhead??.sh files and runs below
python makeConfigs.py configs ${BOTOCORE} ${FLEECE} ${LOCALLIBS} --swbkt ${SPOTBKTWEST} --swbkteast ${SPOTBKTEAST}

#three each: make, West, East for
###nothing / clean (C)
###fleece only (tracing) (T)
###fleece only (tracing+daemon) (F)
###original spotwrap (static insertion of wrapper) (S)
###gammaray (dynamic insertion of wrapper) (D)
python setupApps.py -f configs/configC.json -p ${AWSPROFILE} --deleteAll
python setupApps.py -f configs/configC.json -p ${AWSPROFILE} --no_spotwrap 
python setupApps.py -f configs/configEastC.json -p ${AWSPROFILE} --no_spotwrap 
python setupApps.py -f configs/configT.json -p ${AWSPROFILE} --deleteAll
python setupApps.py -f configs/configT.json -p ${AWSPROFILE} --no_spotwrap --turn_on_tracing
python setupApps.py -f configs/configEastT.json -p ${AWSPROFILE} --no_spotwrap --turn_on_tracing
python setupApps.py -f configs/configF.json -p ${AWSPROFILE} --deleteAll
python setupApps.py -f configs/configF.json -p ${AWSPROFILE} --no_spotwrap --turn_on_tracing --with_fleece
python setupApps.py -f configs/configEastF.json -p ${AWSPROFILE} --no_spotwrap --turn_on_tracing --with_fleece
python setupApps.py -f configs/configS.json -p ${AWSPROFILE} --deleteAll
python setupApps.py -f configs/configS.json -p ${AWSPROFILE} --spotFnsTableName spotFns --spotFnsTableRegion ${REG}
python setupApps.py -f configs/configEastS.json -p ${AWSPROFILE} --spotFnsTableName spotFns --spotFnsTableRegion ${REG}
python setupApps.py -f configs/configD.json -p ${AWSPROFILE} --deleteAll
python setupApps.py -f configs/configD.json -p ${AWSPROFILE} --no_spotwrap --spotFnsTableName gammaRays --spotFnsTableRegion ${REG} --gammaRay
python setupApps.py -f configs/configEastD.json -p ${AWSPROFILE} --no_spotwrap --spotFnsTableName gammaRays --spotFnsTableRegion ${REG} --gammaRay

#only if you want to: blow away the timings directories from past runs, update END to the last directory ID
export END=100
for i in $(seq 1 $END); do rm -rf ${PREFIX}/gammaRay/apps/${i}; done
for i in $(seq 1 $END); do rm -rf ${PREFIX}/gammaRay/apps/map-reduce/${i}; done
for i in $(seq 1 $END); do rm -rf ${PREFIX}/tools/dynamodb/${i}; done
for i in $(seq 1 $END); do rm -rf ${PREFIX}/tools/cloudwatch/${i}; done
for i in $(seq 1 $END); do rm -rf ${PREFIX}/tools/cloudwatch/logs/; done

#download the streams, you will use these to diff out the old stuff when you get new stuff
#update these commands with your stream name
cd ${PREFIX}/tools
python get_stream_data.py arn:aws:dynamodb:${REG}:${ACCT}:table/${SPOTTABLE}/stream/${SPOT_STREAMID} -p ${AWSPROFILE} >& streamS.base
python get_stream_data.py arn:aws:dynamodb:${REG}:${ACCT}:table/${GAMMATABLE}/stream/${GR_STREAMID} -p ${AWSPROFILE} >&  streamD.base

#run synchronized map reduce job overhead measurement (second param is count)
export COUNT=50
nohup ./overheadC.sh ${AWSPROFILE} ${COUNT} ${BDBENCH} ${PREFIX} &
#wait between each at least 12 mins to ensure completion to avoid contention
nohup ./overheadF.sh ${AWSPROFILE} ${COUNT} ${BDBENCH} ${PREFIX} &
#wait between each at least 12 mins to ensure completion to avoid contention
nohup ./overheadT.sh ${AWSPROFILE} ${COUNT} ${BDBENCH} ${PREFIX} &
#wait between each at least 12 mins to ensure completion to avoid contention
nohup ./overheadS.sh ${AWSPROFILE} ${COUNT} ${BDBENCH} ${PREFIX} &
#wait between each at least 12 mins to ensure completion to avoid contention
nohup ./overheadD.sh ${AWSPROFILE} ${COUNT} ${BDBENCH} ${PREFIX} &

#run microbenchmarks (default is 100 times) in gammaRay/micro-benchmarks/ directory
#modify each of the files replace the bucket with $APITESTBKT and the tablenames with $APITESTTABLE
cd ${PREFIX}/tools/timings
#todo: setup triggers

#imageProc app, diff out the differences between the streams and the base and store them in base for next program
./ohead_imgProc.sh ${AWSPROFILE} ${PREFIX}
python ../get_stream_data.py arn:aws:dynamodb:${REG}:${ACCT}:table/${SPOTTABLE}/stream/${SPOT_STREAMID} -p ${AWSPROFILE} >& streamS.new
diff -b -B streamS.base streamS.new| awk -F"> " '{print $2}' > imageProcS.stream
cat imageProcS.stream >> streamS.base
python ../get_stream_data.py arn:aws:dynamodb:${REG}:${ACCT}:table/${GAMMATABLE}/stream/${GR_STREAMID} -p ${AWSPROFILE} >&  streamD.base
diff -b -B streamD.base streamD.new| awk -F"> " '{print $2}' > imageProcD.stream
cat imageProcD.stream >> streamD.base

#microbenchmarks, no streams needed for micro benchmarks
./micro.sh ${AWSPROFILE} ${COUNT} ${PREFIX}

#map-reduce 
./mr.sh ${AWSPROFILE} ${ACCT} ${COUNT} ${PREFIX}

#webapp
./webapp.sh ${AWSPROFILE} ${ACCT} ${COUNT} ${PREFIX}

#dbsync, capture streams for this one
./dbsync.sh ${AWSPROFILE} ${ACCT} ${COUNT} ${PREFIX}
python ../get_stream_data.py arn:aws:dynamodb:${REG}:${ACCT}:table/${SPOTTABLE}/stream/${SPOT_STREAMID} -p ${AWSPROFILE} >& streamS.new
diff -b -B streamS.base streamS.new| awk -F"> " '{print $2}' > dbsyncS.stream
cat dbsyncS.stream >> streamS.base
python ../get_stream_data.py arn:aws:dynamodb:${REG}:${ACCT}:table/${GAMMATABLE}/stream/${GR_STREAMID} -p ${AWSPROFILE} >&  streamD.base
diff -b -B streamD.base streamD.new| awk -F"> " '{print $2}' > dbsyncD.stream
cat dbsyncD.stream >> streamD.base

#process streams to generate pdfs for S (D is the same)
python ../ddb_parser.py dbsyncS.stream
python ../ddb_parser.py imageProcS.stream

#######  CLEANUP ############
#delete everything in AWS, and I mean everything! 
cd ${PREFIX}/tools/timings
#add any other lambdas you want to delete to restConfigsWest.json and restConfigsEast.json
#depending on where your lambdas are
./cleanupAWS.sh ${AWSPROFILE} ${PREFIX}    #you may have to temporarily increased provisioned writes if this hangs or takes a long time until you get to empty!

#you can/should run cleanupDB.sh repeatedly until it says that the table is empty! this can take awhile
./cleanupDB.sh ${AWSPROFILE} ${PREFIX}    #you may have to temporarily increased provisioned writes if this hangs or takes a long time until you get to empty!
#delete all of the lambdas by name, in AWS (edit restConfigs[West,East].json to add others)

./cleanupLambdas.sh ${AWSPROFILE} ${AWSROLE} ${PREFIX}
#you now need to go and run makeConfigs.sh and all of the setupApps.py above
export END=100
for i in $(seq 1 $END); do rm -rf ${PREFIX}/gammaRay/apps/${i}; done
for i in $(seq 1 $END); do rm -rf ${PREFIX}/gammaRay/apps/map-reduce/${i}; done
for i in $(seq 1 $END); do rm -rf ${PREFIX}/tools/dynamodb/${i}; done
for i in $(seq 1 $END); do rm -rf ${PREFIX}/tools/cloudwatch/${i}; done
for i in $(seq 1 $END); do rm -rf ${PREFIX}/tools/cloudwatch/logs/; done

#download the streams, you will use these to diff out the old stuff when you get new stuff
#update these commands with your stream name
python ../get_stream_data.py arn:aws:dynamodb:${REG}:${ACCT}:table/${SPOTTABLE}/stream/${SPOT_STREAMID} -p ${AWSPROFILE} >& streamS.base
python ../get_stream_data.py arn:aws:dynamodb:${REG}:${ACCT}:table/${GAMMATABLE}/stream/${GR_STREAMID} -p ${AWSPROFILE} >&  streamD.base
#now you can start fresh
======================
CASES:
C - nothing/clean
T - tracing
F - tracing + fleece daemon
S - static spotwrap (original)
D - dynamic spotwrap (gammaray)
