git pull
export PREFIX=/Users/ckrintz/RESEARCH/lambda/UCSBFaaS-Wrappers

#update the following with your aws account details
#role with admin rights
export ACCT=443592014519
export ROLE=adminlambda
export AWSRole=arn:aws:iam::${ACCT}:role/${ROLE}
export AWSROLE=arn:aws:iam::${ACCT}:role/${ROLE}
export AWSPROFILE=cjk1
export REG=us-west-2
export XREG=us-east-1
export FLEECEDIR=${PREFIX}/fleece0.13.0
export FLEECE=${FLEECEDIR}/venv/lib/python3.6/site-packages/fleece
export BFLEECEDIR=${PREFIX}/fleece0.13.0lite
export BFLEECE=${BFLEECEDIR}/venv/lib/python3.6/site-packages/fleece
export BOTOCOREDIR=${PREFIX}/boto144
export BOTOCORE=${BOTOCOREDIR}/venv/lib/python3.6/site-packages/botocore
export LOCALLIBDIR=${PREFIX}/gammaRay
export LOCALLIBS=${LOCALLIBDIR}/venv/lib/python3.6/site-packages

export BDBENCH=cjk-gammaray-bdbenchmark
export SPOTTABLE=spotFns
export GAMMATABLE=gammaRays
export SPOTBKTWEST=cjktestbkt
export SPOTBKTEAST=cjktestbkteast

#s3 buckets used by apps and apis
export APITESTBKT=cjk-apitest-bucket
export TRIGGERBKT=cjklambdatrigger

#tables used by apps and apis
export EASTSYNCTABLE=eastSyncTable
export APITESTTABLE=noTriggerTestTable
export WEBAPPTRIGGERTABLE=triggerTable
export IMAGEPROC_DBSYNC=imageLabels

#others explained below
export GR_STREAMID=2017-09-07T18:36:33.398
export SPOT_STREAMID=2017-09-07T18:35:46.003
export MRBKTS=spot-mr-bkt
export MRBKTF=spot-mr-bkt-f
export MRBKTD=spot-mr-bkt-gr
export MRBKTC=spot-mr-bkt-ns
export MRBKTT=spot-mr-bkt-t
export MRBKTB=spot-mr-bkt-b
export END=100
export COUNT=10
export JOBID=job8000 
export MRDIR=${PREFIX}/gammaRay/apps/map-reduce
export CWDIR=${PREFIX}/tools/cloudwatch

#CLEAN_DB
#to start fresh delete them first
aws dynamodb delete-table --table-name ${SPOTTABLE} --profile ${AWSPROFILE}
aws dynamodb delete-table --table-name ${GAMMATABLE} --profile ${AWSPROFILE}
#also for the ones that the apps use
#NOTE That if you delete/recreate these you will need to go in and manually add back in the triggers via Lambda Management console (see TRIGGERS below)
aws dynamodb delete-table --table-name ${EASTSYNCTABLE} --profile ${AWSPROFILE} --region ${XREG}
aws dynamodb delete-table --table-name ${APITESTTABLE} --profile ${AWSPROFILE} --region ${REG}
aws dynamodb delete-table --table-name ${WEBAPPTRIGGERTABLE} --profile ${AWSPROFILE} --region ${REG}
aws dynamodb delete-table --table-name ${IMAGEPROC_DBSYNC} --profile ${AWSPROFILE}
#verify they are gone in the dynamoDB management console (east and west): https://us-west-2.console.aws.amazon.com/dynamodb/home?region=us-west-2

#create tables needed for apps
aws --profile ${AWSPROFILE} dynamodb create-table --region ${XREG} --table-name ${EASTSYNCTABLE} --attribute-definitions AttributeName=id,AttributeType=N --key-schema AttributeName=id,KeyType=HASH --provisioned-throughput ReadCapacityUnits=5,WriteCapacityUnits=5
aws --profile ${AWSPROFILE} dynamodb create-table --region ${REG} --table-name ${APITESTTABLE} --attribute-definitions AttributeName=id,AttributeType=N --key-schema AttributeName=id,KeyType=HASH --provisioned-throughput ReadCapacityUnits=5,WriteCapacityUnits=5
aws --profile ${AWSPROFILE} dynamodb create-table --region ${REG} --table-name ${WEBAPPTRIGGERTABLE} --attribute-definitions AttributeName=id,AttributeType=N --key-schema AttributeName=id,KeyType=HASH --provisioned-throughput ReadCapacityUnits=5,WriteCapacityUnits=5
aws --profile ${AWSPROFILE} dynamodb create-table --region ${REG} --table-name ${IMAGEPROC_DBSYNC} --attribute-definitions AttributeName=id,AttributeType=N --key-schema AttributeName=id,KeyType=HASH --provisioned-throughput ReadCapacityUnits=5,WriteCapacityUnits=5

# make tables needed for GammaRay (S=static=SPOTTABLE and D=dynamic=GAMMATABLE)
aws --profile ${AWSPROFILE} dynamodb create-table --region ${REG} --table-name ${SPOTTABLE} --attribute-definitions AttributeName=reqID,AttributeType=S --key-schema AttributeName=reqID,KeyType=HASH --provisioned-throughput ReadCapacityUnits=10,WriteCapacityUnits=20 --stream-specification StreamEnabled=true,StreamViewType=NEW_AND_OLD_IMAGES
#note the stream ID that comes back from this and use it to set this env var (if you miss it go to the dynamodb management console and get it under the Overview tab)
#just put the date/time component (after .../stream/) from LatestStreamArn below for later use
export SPOT_STREAMID=2017-09-07T18:35:46.003

aws --profile ${AWSPROFILE} dynamodb create-table --region ${REG} --table-name ${GAMMATABLE} --attribute-definitions AttributeName=reqID,AttributeType=S --key-schema AttributeName=reqID,KeyType=HASH --provisioned-throughput ReadCapacityUnits=10,WriteCapacityUnits=20 --stream-specification StreamEnabled=true,StreamViewType=NEW_AND_OLD_IMAGES
#note the stream ID that comes back from this and use it to set this env var (if you miss it go to the dynamodb management console and get it under the Overview tab)
#just put the date/time component (after .../stream/) here for later use
export GR_STREAMID=2017-09-07T18:36:33.398

############## do this set of things only once! ################
#if you haven't done so already, clone the big-data-benchmark to the west; only do this ONCE!
aws --profile ${AWSPROFILE} s3 mb s3://${BDBENCH}
#this takes a super long time!
aws --profile ${AWSPROFILE} s3 sync s3://big-data-benchmark s3://${BDBENCH)
#setup the microbenchmarks
cd ${PREFIX}/gammaRay/micro-benchmarks
#do this ONLY once to fill the table with data (modify the file to set S3BKT=$APITESTBKT and DYNAMODB=$APITESTTABLE)
./prepare.sh ${APITESTBKT} ${APITESTTABLE}
###############################################################

#if you haven't done so already, make the buckets and table
aws --profile ${AWSPROFILE} s3 mb s3://${SPOTBKTWEST}
aws --profile ${AWSPROFILE} s3 mb s3://${SPOTBKTEAST}
aws --profile ${AWSPROFILE} s3 mb s3://${APITESTBKT}
#1 per job type for imageProc app
aws --profile ${AWSPROFILE} s3 mb s3://image-proc-c
aws --profile ${AWSPROFILE} s3 mb s3://image-proc-t
aws --profile ${AWSPROFILE} s3 mb s3://image-proc-s
aws --profile ${AWSPROFILE} s3 mb s3://image-proc-d
aws --profile ${AWSPROFILE} s3 mb s3://image-proc-f
aws --profile ${AWSPROFILE} s3 mb s3://image-proc-b
#1 per job type for map reduce app (mine use old names: ns=C and gr=D now, empty=S)
#must match what is used in gammaRays/makeConfigs.py triggerBuckets datastructure for reducerCoordinator lambdas
export MRBKTS=spot-mr-bkt
export MRBKTF=spot-mr-bkt-f
export MRBKTD=spot-mr-bkt-gr
export MRBKTC=spot-mr-bkt-ns
export MRBKTT=spot-mr-bkt-t
export MRBKTB=spot-mr-bkt-b
aws --profile ${AWSPROFILE} s3 mb s3://${MRBKTS}
aws --profile ${AWSPROFILE} s3 mb s3://${MRBKTF}
aws --profile ${AWSPROFILE} s3 mb s3://${MRBKTD}
aws --profile ${AWSPROFILE} s3 mb s3://${MRBKTC}
aws --profile ${AWSPROFILE} s3 mb s3://${MRBKTT}
aws --profile ${AWSPROFILE} s3 mb s3://${MRBKTB}

deactivate
#$BOTOCORE is passed as first argument to makeConfigs.py
mkdir -p ${BOTOCOREDIR}
cd ${BOTOCOREDIR}
virtualenv venv --python=python3
source venv/bin/activate
pip install --upgrade pip setuptools wheel
pip install 'boto3==1.4.4' --force-reinstall
cd ${BOTOCORE}
patch -b < ${PREFIX}/gammaRay/client144.patch
deactivate

#$FLEECE is passed as second argument to makeConfigs.py
mkdir -p ${FLEECEDIR}
cd ${FLEECEDIR}
virtualenv venv --python=python3
source venv/bin/activate
pip install --upgrade pip setuptools wheel
pip install 'fleece==0.13.0' --force-reinstall
cd ${FLEECE}
patch -b < ${PREFIX}/gammaRay/xray0130.patch
deactivate

#$BFLEECE is passed as second argument to makeConfigs.py
mkdir -p ${BFLEECEDIR}
cd ${BFLEECEDIR}
virtualenv venv --python=python3
source venv/bin/activate
pip install --upgrade pip setuptools wheel
pip install 'fleece==0.13.0' --force-reinstall
cd ${BFLEECE}
patch -b < ${PREFIX}/gammaRay/xray0130lite.patch
deactivate

#$LOCALLIBS is passed as third argument to makeConfigs.py
cd ${LOCALLIBDIR}  #this should be gammaRays directory for most 
virtualenv venv --python=python3
#setup a venv for local testing
source venv/bin/activate
pip install 'fleece==0.13.0' --force-reinstall #fleece installs boto3 the version will work fine for local testing

deactivate
cd ${PREFIX}/tools/timings
#clean up all logs if any
./cleanupAWS.sh ${AWSPROFILE} ${REG} ${XREG}
#Next: clean out the SPOTTABLE and GAMMATABLE databases
#see above (CLEAN_DB) to remove the databases and add them back (with a delay in between to ensure they get deleted)

#Next: delete all of the lambdas by name, in AWS
#modify restConfigsWest.json and restConfigsEast.json to add the names of the other lambdas you want to delete
#note that this deletes everything!  any names that aren't found are just skipped (missing lambda names are not a problem)
./cleanupLambdas.sh ${AWSPROFILE} ${AWSROLE} ${PREFIX}
#check the AWS Lambda management console 
#edit restConfigs[West,East].json to add others

#generate the config files for the lambdas for all cases (CASES list is below)
cd ${PREFIX}/gammaRay
source venv/bin/activate
rm -rf configs
mkdir -p configs
#edit makeConfigs to update triggerBuckets and triggerTables datastructures to have your bucket names
#triggerBuckets for reducerCoordinator must match those in overhead??.sh files and runs below
python makeConfigs.py configs ${BOTOCORE} ${FLEECE} ${LOCALLIBS} --swbkt ${SPOTBKTWEST} --swbkteast ${SPOTBKTEAST} --Bversion ${BFLEECE}
#three each: make, West, East for
###nothing / clean (C)
###fleece only (tracing) (T)
###fleece only (tracing+daemon) (F)
###original spotwrap (static insertion of wrapper) (S)
###gammaray (dynamic insertion of wrapper:tracing_all+timing) (D)
###gammaray (dynamic insertion of wrapper: fleece+minimal_tracing) (B)
python setupApps.py -f configs/configC.json -p ${AWSPROFILE} --deleteAll
python setupApps.py -f configs/configEastC.json -p ${AWSPROFILE} --deleteAll
python setupApps.py -f configs/configT.json -p ${AWSPROFILE} --deleteAll
python setupApps.py -f configs/configEastT.json -p ${AWSPROFILE} --deleteAll
python setupApps.py -f configs/configF.json -p ${AWSPROFILE} --deleteAll
python setupApps.py -f configs/configEastF.json -p ${AWSPROFILE} --deleteAll
python setupApps.py -f configs/configS.json -p ${AWSPROFILE} --deleteAll
python setupApps.py -f configs/configEastS.json -p ${AWSPROFILE} --deleteAll
python setupApps.py -f configs/configD.json -p ${AWSPROFILE} --deleteAll
python setupApps.py -f configs/configEastD.json -p ${AWSPROFILE} --deleteAll
python setupApps.py -f configs/configB.json -p ${AWSPROFILE} --deleteAll
python setupApps.py -f configs/configEastB.json -p ${AWSPROFILE} --deleteAll

python setupApps.py -f configs/configC.json -p ${AWSPROFILE} --no_spotwrap 
python setupApps.py -f configs/configEastC.json -p ${AWSPROFILE} --no_spotwrap 
python setupApps.py -f configs/configT.json -p ${AWSPROFILE} --no_spotwrap --turn_on_tracing
python setupApps.py -f configs/configEastT.json -p ${AWSPROFILE} --no_spotwrap --turn_on_tracing
python setupApps.py -f configs/configF.json -p ${AWSPROFILE} --no_spotwrap --turn_on_tracing --with_fleece
python setupApps.py -f configs/configEastF.json -p ${AWSPROFILE} --no_spotwrap --turn_on_tracing --with_fleece
python setupApps.py -f configs/configS.json -p ${AWSPROFILE} --spotFnsTableName spotFns --spotFnsTableRegion ${REG}
python setupApps.py -f configs/configEastS.json -p ${AWSPROFILE} --spotFnsTableName spotFns --spotFnsTableRegion ${REG}
python setupApps.py -f configs/configD.json -p ${AWSPROFILE} --no_spotwrap --spotFnsTableName gammaRays --spotFnsTableRegion ${REG} --gammaRay
python setupApps.py -f configs/configEastD.json -p ${AWSPROFILE} --no_spotwrap --spotFnsTableName gammaRays --spotFnsTableRegion ${REG} --gammaRay
python setupApps.py -f configs/configB.json -p ${AWSPROFILE} --no_spotwrap --spotFnsTableName gammaRays --spotFnsTableRegion ${REG} --gammaRay --turn_on_tracing
python setupApps.py -f configs/configEastB.json -p ${AWSPROFILE} --no_spotwrap --spotFnsTableName gammaRays --spotFnsTableRegion ${REG} --gammaRay --turn_on_tracing
deactivate

#make the subscriptions for sns topics to trigger S3ModPy (webapp)
cd ${PREFIX}/tools
./make_subs.sh ${AWSPROFILE} ${ACCT} ${REG} 
#to delete them (if ever you need to, go to the SNS Management Console, click Subscriptions and Topics, select and use Action->Delete
#to delete --> also check Lambda Mgmnt Console for each lambda and the triggers tab (& delete if there)

#TRIGGERS  ############## Event Source Setup for apps #######################
Next, add triggers to FnInvokerPy[C,F,D,S,T,B] via the Lambda Management Console for DynamoDB 
Delete any tables that are there that have been previously deleted/recreated ("Problem..." 
will be in the Latest Result) for table ${WEBAPPTRIGGERTABLE}, use trim horizon
--> make it one table per function if you don't want to trigger them all every time. 
Its likely fine to just have one table for all functions, we can make this change later (for a final
version of a paper if accepted). TODO: turn this into a script like make_subs.sh
Next, add permission from WEBAPPTRIGGERTABLE to each FnInvokerPy lambda via:
in the following do this for FnInvokerPyC,D,F,S,T,B
aws lambda add-permission \
--function-name FnInvokerPyC \
--region ${REG} \
--statement-id id1 \
--action "lambda:InvokeFunction" \
--principal s3.amazonaws.com \
--source-arn arn:aws:s3:::${WEBAPPTRIGGERTABLE} \
--source-account ${ACCT} \
--profile ${AWSPROFILE}

Next, add triggers to DBSyncPy[C,F,D,S,T,B] via the Lambda Management Console for DynamoDB for table ${IMAGEPROC_DBSYNC}, trim horizon
--> make it one table per function if you don't want to trigger them all every time -- if you do this, you also need to 
change imageProc to write to the right version of each table depending on its name (eg ImageProcPyC writes to imageProcS table which triggers DBSyncS function),
you will need to add code to imageProc.py to do this.  
Its likely fine to just have one table for all functions, we can make this change later (for a final
version of a paper if accepted). TODO: turn this into a script like make_subs.sh
Next, add permission from IMAGEPROC_DBSYNC to each DBSyncPy lambda via:
in the following do this for DBSyncPyC,D,F,S,T
aws lambda add-permission \
--function-name DBSyncPyC \
--region ${REG} \
--statement-id id1 \
--action "lambda:InvokeFunction" \
--principal s3.amazonaws.com \
--source-arn arn:aws:s3:::${IMAGEPROC_DBSYNC} \
--source-account ${ACCT} \
--profile ${AWSPROFILE}

Next, add triggers to UpdateWebsite[C,F,D,S,T] ***in us-east-1 (${XREG})*** via the Lambda Management Console for DynamoDB for table ${EASTSYNCTABLE}, trim horizon
Its likely fine to just have one table for all functions, we can make this change later (for a final
version of a paper if accepted). TODO: turn this into a script like make_subs.sh
Next, add permission from EASTSYNCTABLE to each UpdateWebsite lambda via:
in the following do this for UpdateWebsiteC,D,F,S,T
aws lambda add-permission \
--function-name UpdateWebsiteC \
--region ${XREG} \
--statement-id id1 \
--action "lambda:InvokeFunction" \
--principal s3.amazonaws.com \
--source-arn arn:aws:s3:::${EASTSYNCTABLE} \
--source-account ${ACCT} \
--profile ${AWSPROFILE}


Next, add a jpg image (any picture of something) to the $SPOTBKTWEST bucket in a folder called imgProc with a file name d1.jpg.  (${SPOTBKTWEST}/imgProc/d1.jpg)
If you change the name or folder, then change this in tools/timings/imgProc.sh which runs the jobs.
We pass this into ImageProcPy apps as input, the image is processed with rekognition in AWS to generate labels.
ImageProcPy apps store the labels in table $IMAGEPROC_DBSYNC (us-west-2)

#only if you want to: blow away the timings directories from past runs, update END to the last directory ID
export END=100
for i in $(seq 1 $END); do rm -rf ${PREFIX}/gammaRay/apps/${i}; done
for i in $(seq 1 $END); do rm -rf ${PREFIX}/gammaRay/apps/map-reduce/${i}; done
for i in $(seq 1 $END); do rm -rf ${PREFIX}/tools/dynamodb/${i}; done
for i in $(seq 1 $END); do rm -rf ${PREFIX}/tools/cloudwatch/${i}; done
for i in $(seq 1 $END); do rm -rf ${PREFIX}/tools/cloudwatch/logs/*; done

#download the streams, you will use these to diff out the old stuff when you get new stuff
#update these commands with your stream name
cd ${PREFIX}/tools
python get_stream_data.py arn:aws:dynamodb:${REG}:${ACCT}:table/${SPOTTABLE}/stream/${SPOT_STREAMID} -p ${AWSPROFILE} >& streamS.base
python get_stream_data.py arn:aws:dynamodb:${REG}:${ACCT}:table/${GAMMATABLE}/stream/${GR_STREAMID} -p ${AWSPROFILE} >&  streamD.base

######################### run the timings!! ################################

#run microbenchmarks (default is 100 times) in gammaRay/micro-benchmarks/ directory
#modify each of the files replace the bucket with $APITESTBKT and the tablenames with $APITESTTABLE
#microbenchmarks, no streams needed for micro benchmarks
cd ${PREFIX}/tools/timings
deactivate
export COUNT=50
./micro.sh ${AWSPROFILE} ${COUNT} ${PREFIX}
./cleanupDB.sh ${AWSPROFILE} ${PREFIX}
./cleanupDB.sh ${AWSPROFILE} ${PREFIX}  #if this keeps saying there are entries, you will need to delete the table (see CLEAN_DB) above)

#map-reduce async
export COUNT=25
cd ${PREFIX}/tools/timings
deactivate
./mr.sh ${AWSPROFILE} ${COUNT} ${BDBENCH} ${PREFIX} ${JOBID} ${MRBKTC} ${MRBKTD} ${MRBKTF} ${MRBKTS} ${MRBKTT} ${MRBKTB} ${REG} 
cd ${LOCALLIBDIR}  #this should be gammaRays directory for most 
source venv/bin/activate
cd ${PREFIX}/tools  #download the stream data and append it to stream base (save both)
python get_stream_data.py arn:aws:dynamodb:${REG}:${ACCT}:table/${SPOTTABLE}/stream/${SPOT_STREAMID} -p ${AWSPROFILE} >& streamS.new
diff -b -B streamS.base streamS.new| awk -F"> " '{print $2}' > mrS.stream
cat mrS.stream >> streamS.base
python get_stream_data.py arn:aws:dynamodb:${REG}:${ACCT}:table/${GAMMATABLE}/stream/${GR_STREAMID} -p ${AWSPROFILE} >&  streamD.base
diff -b -B streamD.base streamD.new| awk -F"> " '{print $2}' > mrD.stream
cat mrD.stream >> streamD.base
deactivate
cd ${PREFIX}/tools/timings
./cleanupDB.sh ${AWSPROFILE} ${PREFIX}

#webapp
cd ${PREFIX}/tools/timings
deactivate
./webapp.sh ${AWSPROFILE} ${COUNT} ${PREFIX} ${REG} ${ACCT} ${TRIGGERBKT}
cd ${LOCALLIBDIR}  #this should be gammaRays directory for most 
source venv/bin/activate
cd ${PREFIX}/tools  #download the stream data and append it to stream base (save both)
python get_stream_data.py arn:aws:dynamodb:${REG}:${ACCT}:table/${SPOTTABLE}/stream/${SPOT_STREAMID} -p ${AWSPROFILE} >& streamS.new
diff -b -B streamS.base streamS.new| awk -F"> " '{print $2}' > webappS.stream
cat webappS.stream >> streamS.base
python get_stream_data.py arn:aws:dynamodb:${REG}:${ACCT}:table/${GAMMATABLE}/stream/${GR_STREAMID} -p ${AWSPROFILE} >&  streamD.base
diff -b -B streamD.base streamD.new| awk -F"> " '{print $2}' > webappD.stream
cat webappD.stream >> streamD.base
deactivate
./cleanupDB.sh ${AWSPROFILE} ${PREFIX}

#imageProc (and dbsync), capture streams for this one
cd ${PREFIX}/tools/timings
deactivate
./imageProc.sh ${AWSPROFILE} ${COUNT} ${PREFIX} ${REG} ${XREG}
cd ${LOCALLIBDIR}  #this should be gammaRays directory for most 
source venv/bin/activate
cd ${PREFIX}/tools  #download the stream data and append it to stream base (save both)
python get_stream_data.py arn:aws:dynamodb:${REG}:${ACCT}:table/${SPOTTABLE}/stream/${SPOT_STREAMID} -p ${AWSPROFILE} >& streamS.new
diff -b -B streamS.base streamS.new| awk -F"> " '{print $2}' > imageProcS.stream
cat imageProcS.stream >> streamS.base
python get_stream_data.py arn:aws:dynamodb:${REG}:${ACCT}:table/${GAMMATABLE}/stream/${GR_STREAMID} -p ${AWSPROFILE} >&  streamD.base
diff -b -B streamD.base streamD.new| awk -F"> " '{print $2}' > imageProcD.stream
cat imageProcD.stream >> streamD.base
deactivate
./cleanupDB.sh ${AWSPROFILE} ${PREFIX}

cd ${PREFIX}/tools/timings
deactivate
#run synchronized map reduce job overhead measurement (second param is count, BDBENCH should be the name of your cloned big-data-benchmark folder)
export COUNT=25
export JOBID=job8000 #must match what is used in makeConfigs triggerBuckets datastructure for reducerCoordinator lambdas (currently job8000 which you can keep if you like)
nohup ./overheadMR.sh ${AWSPROFILE} ${COUNT} ${BDBENCH} ${PREFIX} ${JOBID} ${MRBKTC} ${MRBKTD} ${MRBKTF} ${MRBKTS} ${MRBKTT} ${MRBKTB}&
./cleanupDB.sh ${AWSPROFILE} ${PREFIX}

############################## process data #############################
#summarize in the excel spreadsheet called result.xlxs in the paper repo
#commit all of the output files to the paper repo under results

#process timings data (cloudwatch)
export COUNT=50 #if unset
cd ${LOCALLIBDIR}  #this should be gammaRays directory for most 
source venv/bin/activate
cd ${PREFIX}/tools
export MRDIR=${PREFIX}/gammaRay/apps/map-reduce
export CWDIR=${PREFIX}/tools/cloudwatch
python timings_parser.py ${MRDIR} ${CWDIR} out --micro_only --count ${COUNT} >micro.summary
#data is in out_jobname.out, e.g. out_s3writeS.out
deactivate

#process streams to generate pdfs for S (D is the same)
cd ${PREFIX}/tools
cd ${LOCALLIBDIR}  #this should be gammaRays directory for most 
source venv/bin/activate
python ddb_parser.py dbsyncS.stream
python ddb_parser.py imageProcS.stream
deactivate

#######  TROUBLESHOOTING ############
- if there are problems send Chandra the details and she can help until you get this all setup
- to see the logs after a run, remove the --delete from the downloadlogs.py calls and they will stay in cloudwatch for you to interrogate


#######  CLEANUP ############
#delete everything in AWS, and I mean everything! 
cd ${PREFIX}/tools/timings
#add any other lambdas you want to delete to restConfigsWest.json and restConfigsEast.json
#depending on where your lambdas are
./cleanupAWS.sh ${AWSPROFILE} ${REG} ${XREG}

#you can/should run cleanupDB.sh repeatedly until it says that the table is empty! this can take awhile
./cleanupDB.sh ${AWSPROFILE} ${PREFIX}    #you may have to temporarily increased provisioned writes if this hangs or takes a long time until you get to empty!
#if there are too many entries, the above will stop working. In this case just delete and remake the table:
aws dynamodb delete-table --table-name ${SPOTTABLE} --profile ${AWSPROFILE}
aws dynamodb delete-table --table-name ${GAMMATABLE} --profile ${AWSPROFILE}
#wait 5 mins then recreate them:
aws --profile ${AWSPROFILE} dynamodb create-table --region ${REG} --table-name ${SPOTTABLE} --attribute-definitions AttributeName=reqID,AttributeType=S --key-schema AttributeName=reqID,KeyType=HASH --provisioned-throughput ReadCapacityUnits=10,WriteCapacityUnits=20 --stream-specification StreamEnabled=true,StreamViewType=NEW_AND_OLD_IMAGES
#note the stream ID that comes back from this and use it to set this env var (if you miss it go to the dynamodb management console and get it under the Overview tab)
#just put the date/time component (after .../stream/) here for later use
export SPOT_STREAMID=2017-09-06T21:22:39.210
aws --profile ${AWSPROFILE} dynamodb create-table --region ${REG} --table-name ${GAMMATABLE} --attribute-definitions AttributeName=reqID,AttributeType=S --key-schema AttributeName=reqID,KeyType=HASH --provisioned-throughput ReadCapacityUnits=10,WriteCapacityUnits=20 --stream-specification StreamEnabled=true,StreamViewType=NEW_AND_OLD_IMAGES
#note the stream ID that comes back from this and use it to set this env var (if you miss it go to the dynamodb management console and get it under the Overview tab)
#just put the date/time component (after .../stream/) here for later use
export GR_STREAMID=2017-09-06T21:23:44.758

#delete the others if needed (steps to remake them are at the top
aws dynamodb delete-table --table-name ${EASTSYNCTABLE} --profile ${AWSPROFILE} --region ${XREG} 
aws dynamodb delete-table --table-name ${APITESTTABLE} --profile ${AWSPROFILE} --region ${REG}
aws dynamodb delete-table --table-name ${WEBAPPTRIGGERTABLE} --profile ${AWSPROFILE} --region ${REG}

#delete all of the lambdas by name, in AWS (edit restConfigs[West,East].json to add others)
./cleanupLambdas.sh ${AWSPROFILE} ${AWSROLE} ${PREFIX}
#you now need to go and run makeConfigs.sh and all of the setupApps.py above
export END=100
for i in $(seq 1 $END); do rm -rf ${PREFIX}/gammaRay/apps/${i}; done
for i in $(seq 1 $END); do rm -rf ${PREFIX}/gammaRay/apps/map-reduce/${i}; done
for i in $(seq 1 $END); do rm -rf ${PREFIX}/tools/dynamodb/${i}; done
for i in $(seq 1 $END); do rm -rf ${PREFIX}/tools/cloudwatch/${i}; done
for i in $(seq 1 $END); do rm -rf ${PREFIX}/tools/cloudwatch/logs/; done

#download the streams, you will use these to diff out the old stuff when you get new stuff
#update these commands with your stream name
python ../get_stream_data.py arn:aws:dynamodb:${REG}:${ACCT}:table/${SPOTTABLE}/stream/${SPOT_STREAMID} -p ${AWSPROFILE} >& streamS.base
python ../get_stream_data.py arn:aws:dynamodb:${REG}:${ACCT}:table/${GAMMATABLE}/stream/${GR_STREAMID} -p ${AWSPROFILE} >&  streamD.base
#now you can start fresh
======================
CASES:
C - nothing/clean
T - tracing
F - tracing + fleece daemon
S - static spotwrap (original)
D - dynamic spotwrap (gammaray)
