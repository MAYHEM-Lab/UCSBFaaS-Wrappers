git pull
export PREFIX=/Users/ckrintz/RESEARCH/lambda/UCSBFaaS-Wrappers

#update the following with your aws account details
#role with admin rights
export ACCT=443592014519
export ROLE=adminlambda
export AWSRole=arn:aws:iam::${ACCT}:role/${ROLE}
export AWSROLE=arn:aws:iam::${ACCT}:role/${ROLE}
export AWSPROFILE=cjk1
export REG=us-west-2
export XREG=us-east-1
export FLEECEDIR=${PREFIX}/fleece0.13.0
export FLEECE=${FLEECEDIR}/venv/lib/python3.6/site-packages/fleece
export BFLEECEDIR=${PREFIX}/fleece0.13.0lite
export BFLEECE=${BFLEECEDIR}/venv/lib/python3.6/site-packages/fleece
export BOTOCOREDIR=${PREFIX}/boto144
export BOTOCORE=${BOTOCOREDIR}/venv/lib/python3.6/site-packages/botocore
export LOCALLIBDIR=${PREFIX}/gammaRay
export LOCALLIBS=${LOCALLIBDIR}/venv/lib/python3.6/site-packages
export MRDIR=${PREFIX}/gammaRay/apps/map-reduce
export CWDIR=${PREFIX}/tools/cloudwatch

export BDBENCH=cjk-gammaray-bdbenchmark
export SPOTTABLE=spotFns
export GAMMATABLE=gammaRays
export SPOTBKTWEST=cjktestbkt
export SPOTBKTEAST=cjktestbkteast

#s3 bucket and dynamodb table used by apis
export APITESTBKT=cjk-apitest-bucket
export APITESTTABLE=noTriggerTestTable

#others explained below
export GR_STREAMID=2017-09-13T00:39:21.716  #set elsewhere in this file also!
export SPOT_STREAMID=2017-09-13T00:43:24.435  #set elsewhere in this file also!
export END=100
export COUNT=10
export JOBID=job8000 

#CLEAN_DB
#to start fresh delete them first
aws dynamodb delete-table --table-name ${SPOTTABLE} --profile ${AWSPROFILE}
aws dynamodb delete-table --table-name ${GAMMATABLE} --profile ${AWSPROFILE}
aws dynamodb delete-table --table-name ${APITESTTABLE} --profile ${AWSPROFILE} --region ${REG}

#now remake them
# make tables needed for GammaRay (S=static=SPOTTABLE and D=dynamic=GAMMATABLE)
aws --profile ${AWSPROFILE} dynamodb create-table --region ${REG} --table-name ${SPOTTABLE} --attribute-definitions AttributeName=reqID,AttributeType=S --key-schema AttributeName=reqID,KeyType=HASH --provisioned-throughput ReadCapacityUnits=10,WriteCapacityUnits=20 --stream-specification StreamEnabled=true,StreamViewType=NEW_AND_OLD_IMAGES
#note the stream ID that comes back from this and use it to set this env var (if you miss it go to the dynamodb management console and get it under the Overview tab)
#just put the date/time component (after .../stream/) from LatestStreamArn below for later use
export SPOT_STREAMID=2017-09-13T00:43:24.435  #set elsewhere in this file also!

aws --profile ${AWSPROFILE} dynamodb create-table --region ${REG} --table-name ${GAMMATABLE} --attribute-definitions AttributeName=reqID,AttributeType=S --key-schema AttributeName=reqID,KeyType=HASH --provisioned-throughput ReadCapacityUnits=10,WriteCapacityUnits=20 --stream-specification StreamEnabled=true,StreamViewType=NEW_AND_OLD_IMAGES
#note the stream ID that comes back from this and use it to set this env var (if you miss it go to the dynamodb management console and get it under the Overview tab)
#just put the date/time component (after .../stream/) here for later use
export GR_STREAMID=2017-09-13T00:39:21.716  #set elsewhere in this file also!

#create tables needed for apis
#primary key is 'id':'N' for all other tables its type is 'S'
aws --profile ${AWSPROFILE} dynamodb create-table --region ${REG} --table-name ${APITESTTABLE} --attribute-definitions AttributeName=id,AttributeType=N --key-schema AttributeName=id,KeyType=HASH --provisioned-throughput ReadCapacityUnits=5,WriteCapacityUnits=5

#if you haven't done so already, make the buckets and table
aws --profile ${AWSPROFILE} s3 mb s3://${SPOTBKTWEST}
aws --profile ${AWSPROFILE} s3 mb s3://${SPOTBKTEAST}
aws --profile ${AWSPROFILE} s3 mb s3://${APITESTBKT}

############## do this set of things only once! ################
#if you haven't done so already, clone the big-data-benchmark to the west; only do this ONCE!
aws --profile ${AWSPROFILE} s3 mb s3://${BDBENCH}
#this takes a super long time!
aws --profile ${AWSPROFILE} s3 sync s3://big-data-benchmark s3://${BDBENCH)
#setup the microbenchmarks
cd ${PREFIX}/gammaRay/micro-benchmarks
#do this ONLY once to fill the table with data (modify the file to set S3BKT=$APITESTBKT and DYNAMODB=$APITESTTABLE)
./prepare.sh ${APITESTBKT} ${APITESTTABLE}
###############################################################

#$BOTOCORE is passed as first argument to makeConfigs.py
deactivate
mkdir -p ${BOTOCOREDIR}
cd ${BOTOCOREDIR}
virtualenv venv --python=python3
source venv/bin/activate
pip install --upgrade pip setuptools wheel
pip install 'boto3==1.4.4' --force-reinstall
cd ${BOTOCORE}
patch -b < ${PREFIX}/gammaRay/client144.patch
deactivate

#$FLEECE is passed as second argument to makeConfigs.py
mkdir -p ${FLEECEDIR}
cd ${FLEECEDIR}
virtualenv venv --python=python3
source venv/bin/activate
pip install --upgrade pip setuptools wheel
pip install 'fleece==0.13.0' --force-reinstall
cd ${FLEECE}
patch -b < ${PREFIX}/gammaRay/xray0130.patch
deactivate

#$BFLEECE is passed as --Bversion argument to makeConfigs.py
mkdir -p ${BFLEECEDIR}
cd ${BFLEECEDIR}
virtualenv venv --python=python3
source venv/bin/activate
pip install --upgrade pip setuptools wheel
pip install 'fleece==0.13.0' --force-reinstall
cd ${BFLEECE}
patch -b < ${PREFIX}/gammaRay/xray0130lite.patch
deactivate

#$LOCALLIBS is passed as third argument to makeConfigs.py
cd ${LOCALLIBDIR}  #this should be gammaRays directory for most 
virtualenv venv --python=python3
#setup a venv for local testing
source venv/bin/activate
pip install 'fleece==0.13.0' --force-reinstall #fleece installs boto3 the version will work fine for local testing

#clean up all logs if any
deactivate
cd ${PREFIX}/tools/timings
./cleanupAWS.sh ${AWSPROFILE} ${REG} ${XREG}
#Next: clean out the SPOTTABLE and GAMMATABLE databases
#see above (CLEAN_DB) to remove the databases and add them back (with a delay in between to ensure they get deleted)

#Next (optional): delete all of the lambdas by name, in AWS
#note that this deletes everything!  any names that aren't 
#found are just skipped (missing lambda names are not a problem)
deactivate
cd ${PREFIX}/tools/timings
./cleanupLambdas.sh ${AWSPROFILE} ${AWSROLE} ${PREFIX}
#check the AWS Lambda management console 
#edit restConfigs[West,East].json and rerun to delete missed lambdas

#clean out the DB
deactivate
cd ${PREFIX}/tools/timings
./cleanupDB.sh ${AWSPROFILE} ${PREFIX}


#next remove all past triggers (add function names to the colon delimited string argument as you wish):
#any functions not found are just skipped
cd ${PREFIX}/gammaRay
source venv/bin/activate
cd ../tools
python cleanupEventSources.py ${AWSPROFILE} ${XREG} "UpdateWebsiteF:UpdateWebsiteT:UpdateWebsiteS:UpdateWebsiteC:UpdateWebsiteB:UpdateWebsiteD:UpdateWebsite"
python cleanupEventSources.py ${AWSPROFILE} ${REG} "DBSyncPyC:DBSyncPyD:DBSyncPyF:DBSyncPyT:DBSyncPyB:DBSyncPyS:FnInvokerPyC:FnInvokerPyD:FnInvokerPyF:FnInvokerPyT:FnInvokerPyB:FnInvokerPyS:DBSync:SpotTemplatePy:FnInvokerPyNS:FnInvokerPy"
deactivate

############ make the lambdas ############################
1) first setup the triggers
#Map-reduce setup: bucket that triggers reducerCoordinator (written to by mapper and reducer)
#The prefix can stay job8000 (if you change this you need to change mr.sh and overheadMR.sh in tools/timings)
export MR_TRIGGERBKTS=spot-mr-bkt
export MR_TRIGGERBKTF=spot-mr-bkt-f
export MR_TRIGGERBKTD=spot-mr-bkt-gr
export MR_TRIGGERBKTC=spot-mr-bkt-ns
export MR_TRIGGERBKTT=spot-mr-bkt-t
export MR_TRIGGERBKTB=spot-mr-bkt-b
#create the buckets if they aren't already created, you can ignore BucketAlreadyOwnedByYou errors
aws --profile ${AWSPROFILE} s3 mb s3://${MR_TRIGGERBKTC}
aws --profile ${AWSPROFILE} s3 mb s3://${MR_TRIGGERBKTT}
aws --profile ${AWSPROFILE} s3 mb s3://${MR_TRIGGERBKTF}
aws --profile ${AWSPROFILE} s3 mb s3://${MR_TRIGGERBKTD}
aws --profile ${AWSPROFILE} s3 mb s3://${MR_TRIGGERBKTS}
aws --profile ${AWSPROFILE} s3 mb s3://${MR_TRIGGERBKTB}
#webapp
export FNI_TRIGGERBKT=cjk-fninvtrigger #without the suffix, just a name, don't create a bucket for it
export FNI_TRIGGERBKTF=cjk-fninvtrigger-f
export FNI_TRIGGERBKTC=cjk-fninvtrigger-c
export FNI_TRIGGERBKTD=cjk-fninvtrigger-d
export FNI_TRIGGERBKTS=cjk-fninvtrigger-s
export FNI_TRIGGERBKTT=cjk-fninvtrigger-t
export FNI_TRIGGERBKTB=cjk-fninvtrigger-b
#make them, you can ignore BucketAlreadyOwnedByYou errors
aws --profile ${AWSPROFILE} s3 mb s3://${FNI_TRIGGERBKTF}
aws --profile ${AWSPROFILE} s3 mb s3://${FNI_TRIGGERBKTC}
aws --profile ${AWSPROFILE} s3 mb s3://${FNI_TRIGGERBKTD}
aws --profile ${AWSPROFILE} s3 mb s3://${FNI_TRIGGERBKTS}
aws --profile ${AWSPROFILE} s3 mb s3://${FNI_TRIGGERBKTT}
aws --profile ${AWSPROFILE} s3 mb s3://${FNI_TRIGGERBKTB}
#imageProc
export IMG_TRIGGERBKTF=image-proc-f
export IMG_TRIGGERBKTC=image-proc-c
export IMG_TRIGGERBKTD=image-proc-d
export IMG_TRIGGERBKTS=image-proc-s
export IMG_TRIGGERBKTT=image-proc-t
export IMG_TRIGGERBKTB=image-proc-b
#make them, you can ignore BucketAlreadyOwnedByYou errors
aws --profile ${AWSPROFILE} s3 mb s3://${IMG_TRIGGERBKTF}
aws --profile ${AWSPROFILE} s3 mb s3://${IMG_TRIGGERBKTC}
aws --profile ${AWSPROFILE} s3 mb s3://${IMG_TRIGGERBKTD}
aws --profile ${AWSPROFILE} s3 mb s3://${IMG_TRIGGERBKTS}
aws --profile ${AWSPROFILE} s3 mb s3://${IMG_TRIGGERBKTT}
aws --profile ${AWSPROFILE} s3 mb s3://${IMG_TRIGGERBKTB}

2) make the configuration files (json)
#generate the config files for the lambdas for all cases (CASES list is below)
cd ${PREFIX}/gammaRay
source venv/bin/activate
rm -rf configs
mkdir -p configs
#edit makeConfigs to update triggerBuckets and triggerTables datastructures to have your bucket names
#triggerBuckets for reducerCoordinator must match those in overhead??.sh files and runs below
python makeConfigs.py configs ${BOTOCORE} ${FLEECE} ${LOCALLIBS} --swbkt ${SPOTBKTWEST} --swbkteast ${SPOTBKTEAST} --Bversion ${BFLEECE}

3) make the lambdas
#three each: make, West, East for
###nothing / clean (C)
###fleece only (tracing) (T)
###fleece only (tracing+daemon) (F)
###original spotwrap (static insertion of wrapper) (S)
###gammaray (dynamic insertion of wrapper:tracing_all+timing) (D)
###gammaray (dynamic insertion of wrapper: fleece+minimal_tracing) (B)
python setupApps.py -f configs/configC.json -p ${AWSPROFILE} --deleteAll
python setupApps.py -f configs/configEastC.json -p ${AWSPROFILE} --deleteAll
python setupApps.py -f configs/configT.json -p ${AWSPROFILE} --deleteAll
python setupApps.py -f configs/configEastT.json -p ${AWSPROFILE} --deleteAll
python setupApps.py -f configs/configF.json -p ${AWSPROFILE} --deleteAll
python setupApps.py -f configs/configEastF.json -p ${AWSPROFILE} --deleteAll
python setupApps.py -f configs/configS.json -p ${AWSPROFILE} --deleteAll
python setupApps.py -f configs/configEastS.json -p ${AWSPROFILE} --deleteAll
python setupApps.py -f configs/configD.json -p ${AWSPROFILE} --deleteAll
python setupApps.py -f configs/configEastD.json -p ${AWSPROFILE} --deleteAll
python setupApps.py -f configs/configB.json -p ${AWSPROFILE} --deleteAll
python setupApps.py -f configs/configEastB.json -p ${AWSPROFILE} --deleteAll

python setupApps.py -f configs/configC.json -p ${AWSPROFILE} --no_spotwrap 
python setupApps.py -f configs/configEastC.json -p ${AWSPROFILE} --no_spotwrap 
python setupApps.py -f configs/configT.json -p ${AWSPROFILE} --no_spotwrap --turn_on_tracing
python setupApps.py -f configs/configEastT.json -p ${AWSPROFILE} --no_spotwrap --turn_on_tracing
python setupApps.py -f configs/configF.json -p ${AWSPROFILE} --no_spotwrap --turn_on_tracing --with_fleece
python setupApps.py -f configs/configEastF.json -p ${AWSPROFILE} --no_spotwrap --turn_on_tracing --with_fleece
python setupApps.py -f configs/configS.json -p ${AWSPROFILE} --spotFnsTableName spotFns --spotFnsTableRegion ${REG}
python setupApps.py -f configs/configEastS.json -p ${AWSPROFILE} --spotFnsTableName spotFns --spotFnsTableRegion ${REG}
python setupApps.py -f configs/configD.json -p ${AWSPROFILE} --no_spotwrap --spotFnsTableName gammaRays --spotFnsTableRegion ${REG} --gammaRay
python setupApps.py -f configs/configEastD.json -p ${AWSPROFILE} --no_spotwrap --spotFnsTableName gammaRays --spotFnsTableRegion ${REG} --gammaRay
python setupApps.py -f configs/configB.json -p ${AWSPROFILE} --no_spotwrap --spotFnsTableName gammaRays --spotFnsTableRegion ${REG} --gammaRay --turn_on_tracing
python setupApps.py -f configs/configEastB.json -p ${AWSPROFILE} --no_spotwrap --spotFnsTableName gammaRays --spotFnsTableRegion ${REG} --gammaRay --turn_on_tracing
deactivate

############# setup the apps #################
#############map-reduce
#nothing to do here (setupApps has done it all)

#############webapp:
what it does:
tools/timings/webapp.sh:SNSPy (topic_) -> 
	triggers S3ModPy_ passes in via the SNS message:
        bkt:${FNI_TRIGGERBKT_}, fname:xxx, prefix:pref_ (_=[CBDSTF]) anywhere in message
        S3Mod writes ${FNI_TRIGGERBKT_}
        --> which triggers FnInvokerPy_ which invokes DBModPy_ -> 
        DBMod reads testTable and writes ${WEBAPP_DBMOD_TRIGGER_TABLE_PREFIX_}  for _=[CBDSTF]
        (see FnInvoker.py for invoke call with params), 
        the write to ${WEBAPP_DBMOD_TRIGGER_TABLE_PREFIX_}->triggers FnInvokerPy (does not invoke DBModPy)
setup: (setupApps.py takes care of setting up FNInvokerPy_ triggers to FNI_TRIGGERBKT_)
1)make the subscriptions for SNS topics to trigger S3ModPy (for webapp) 
cd ${PREFIX}/tools
./make_subs.sh ${AWSPROFILE} ${ACCT} ${REG} 
#to delete them (if ever you need to, go to the SNS Management Console, click Subscriptions and Topics, select and use Action->Delete
#to delete --> also check Lambda Mgmnt Console for each lambda and the triggers tab (& delete if there)

2) add dynamoDB trigger for ${WEBAPP_DBMOD_TRIGGER_TABLE_PREFIX_} for _=[CDSFTB] to all FnInvokerPy_ lambdas (from DBModPy_ DB write)
deactivate
cd ${PREFIX}/tools
export WEBAPP_DBMOD_TRIGGER_TABLE_PREFIX=triggerTable- #if you change this you must edit gammaRay/apps/FnInvoker.py (grep for DBMod) and change it there as well
export WEBAPP_DBMOD_FUNCTION_NAME_PREFIX=FnInvokerPy
./make_imgproc_tables.sh ${WEBAPP_DBMOD_TRIGGER_TABLE_PREFIX} ${WEBAPP_DBMOD_FUNCTION_NAME_PREFIX} ${AWSPROFILE} ${REG} ${PREFIX}

3) test that there are 5 functions triggered (look at Xray and Cloudwatch logs): SNSPyF,S3ModF,FnInvokerPyF(log message about DBModPyF invoke),DBModPyF, FnInvokerPyF (msg=post to webpage)
#change the suffix on SNSPy, topic, pref, and FNI_TRIGGERBKT to change to a config other than F
aws lambda invoke --invocation-type Event --function-name SNSPyF --region ${REG} --profile ${AWSPROFILE} --payload "{\"eventSource\":\"ext:invokeCLI\",\"topic\":\"arn:aws:sns:${REG}:${ACCT}:topicF\",\"subject\":\"sub1\",\"msg\":\"fname:testfile.txt:prefix:prefF:bkt:${FNI_TRIGGERBKTF}:xxx\"}" outputfile

###########imageProc 
tools/timings/imageProc.sh: ImgProc_ -> CLI Invoked calls http, rekognition, ${IMAGEPROC_DBSYNC} DB table write
        ${IMAGEPROC_DBSYNC} write triggers DBSyncPy (all of them which is fine b/c we only download the _ log)
        DBSyncPy writes ${EASTSYNCTABLE} in east region
        UpdateWebsite (all of them) in east is triggered by ${EASTSYNCTABLE} write and 
        invokes http
#imageProc setup (setupApps has setup the bucket triggers)
1) [Optional] Edit DBSync.py to change the hardcoded string "eastSyncTable-" 
(which is EASTSYNC_TRIGGER_TABLE_PREFIX value below) if you would 
like to change the table name to something else.  However you can leave it
as is and everything should work (since tablenames do not need to be unique across users like 
s3 bucket names do).

2) Add triggers to DBSyncPy[C,F,D,S,T,B] from associated IMG_DBSYNC_TRIGGER_TABLE 
tables in the ${REG} region:
deactivate
cd ${PREFIX}/tools
export IMG_DBSYNC_TRIGGER_TABLE_PREFIX=image-proc- #if you change this you must edit gammaRay/apps/DBSync.py and change it there as well
export IMG_DBSYNC_FUNCTION_NAME_PREFIX=DBSyncPy
./make_imgproc_tables.sh ${IMG_DBSYNC_TRIGGER_TABLE_PREFIX} ${IMG_DBSYNC_FUNCTION_NAME_PREFIX} ${AWSPROFILE} ${REG} ${PREFIX}

3) Next, add triggers to UpdateWebsite[C,F,D,S,T,B] in the *** ${XREG} region ***
deactivate
cd ${PREFIX}/tools
export EASTSYNC_TRIGGER_TABLE_PREFIX=eastSyncTable-
export EASTSYNC_FUNCTION_NAME_PREFIX=UpdateWebsite
./make_imgproc_tables.sh ${EASTSYNC_TRIGGER_TABLE_PREFIX} ${EASTSYNC_FUNCTION_NAME_PREFIX} ${AWSPROFILE} ${XREG} ${PREFIX}

4) Next, upload a jpg image (any picture of something) to the $SPOTBKTWEST bucket in a folder 
called imgProc with a file name d1.jpg.  (${SPOTBKTWEST}/imgProc/d1.jpg) (update timings/imageProc.sh if you change prefix and/or key)
# we use this to simulate a user dropping an image into a webapp or bucket that triggers the lambdas,
#however in imageProc.sh we just invoke the function directly passing in this file, for simplicity
#it is setup and does work if an image is dropped into th image-proc-?/pref?/ buckets/folders above
If you change the name or folder, then change this in tools/timings/imgProc.sh which runs the jobs.
We pass this into ImageProcPy apps as input, the image is processed with rekognition in AWS to generate labels.

5) test that there are 5 functions triggered (look at Xray and Cloudwatch logs): ImageProcPyF,DBSyncPyF,UpdateWebsiteF in ${XREG} region
#change the suffix on ImageProcPy and after ${IMG_DBSYNC_TRIGGER_TABLE_PREFIX} to change the config from F
aws lambda invoke --invocation-type Event --function-name ImageProcPyF --region ${REG} --profile ${AWSPROFILE} --payload "{\"eventSource\":\"ext:invokeCLI\",\"name\":\"${SPOTBKTWEST}\",\"key\":\"imgProc/d1.jpg\",\"tableName\":\"${IMG_DBSYNC_TRIGGER_TABLE_PREFIX}F\"}" outputfile
#You should see successful execution of: ImageProcPyF(in west)->DBSyncPyF(in west)->UpdateWebsiteF(in east)

#################### setup to run the experiments **************************
#only if you want to: blow away the timings directories from past runs, update END to the last directory ID
#perhaps you want to move them first to backup what you have...
export END=100
for i in $(seq 1 $END); do rm -rf ${PREFIX}/gammaRay/apps/${i}; done
for i in $(seq 1 $END); do rm -rf ${PREFIX}/gammaRay/apps/map-reduce/${i}; done
for i in $(seq 1 $END); do rm -rf ${PREFIX}/tools/dynamodb/${i}; done
for i in $(seq 1 $END); do rm -rf ${PREFIX}/tools/cloudwatch/${i}; done
for i in $(seq 1 $END); do rm -rf ${PREFIX}/tools/cloudwatch/logs/*; done

#download the streams, you will use these to diff out the old stuff when you get new stuff
#update these commands with your stream name
deactivate
cd ${PREFIX}/tools/timings
./cleanupDB.sh ${AWSPROFILE} ${PREFIX}
cd ${PREFIX}/gammaRay
source venv/bin/activate
cd ${PREFIX}/tools
python get_stream_data.py arn:aws:dynamodb:${REG}:${ACCT}:table/${SPOTTABLE}/stream/${SPOT_STREAMID} -p ${AWSPROFILE} >& streamS.base
python get_stream_data.py arn:aws:dynamodb:${REG}:${ACCT}:table/${GAMMATABLE}/stream/${GR_STREAMID} -p ${AWSPROFILE} >&  streamD.base
deactivate
######################### run the timings!! ################################

#run microbenchmarks (default is 100 times) in gammaRay/micro-benchmarks/ directory
#modify each of the files replace the bucket with $APITESTBKT and the tablenames with $APITESTTABLE
#microbenchmarks, no streams needed for micro benchmarks
cd ${PREFIX}/tools/timings
deactivate
export COUNT=50
./micro.sh ${AWSPROFILE} ${COUNT} ${PREFIX}
./cleanupDB.sh ${AWSPROFILE} ${PREFIX}
./cleanupDB.sh ${AWSPROFILE} ${PREFIX}  #if this keeps saying there are entries, you will need to delete the table (see CLEAN_DB) above)

#map-reduce async
export COUNT=25
cd ${PREFIX}/tools/timings
deactivate
./mr.sh ${AWSPROFILE} ${COUNT} ${BDBENCH} ${PREFIX} ${JOBID} ${MR_TRIGGERBKTC} ${MR_TRIGGERBKTD} ${MR_TRIGGERBKTF} ${MR_TRIGGERBKTS} ${MR_TRIGGERBKTT} ${MR_TRIGGERBKTB} ${REG} 
cd ${LOCALLIBDIR}  #this should be gammaRays directory for most 
source venv/bin/activate
cd ${PREFIX}/tools  #download the stream data and append it to stream base (save both)
python get_stream_data.py arn:aws:dynamodb:${REG}:${ACCT}:table/${SPOTTABLE}/stream/${SPOT_STREAMID} -p ${AWSPROFILE} >& streamS.new
diff -b -B streamS.base streamS.new| awk -F"> " '{print $2}' > mrS.stream
cat mrS.stream >> streamS.base
python get_stream_data.py arn:aws:dynamodb:${REG}:${ACCT}:table/${GAMMATABLE}/stream/${GR_STREAMID} -p ${AWSPROFILE} >&  streamD.new
diff -b -B streamD.base streamD.new| awk -F"> " '{print $2}' > mrD.stream
cat mrD.stream >> streamD.base
deactivate
cd ${PREFIX}/tools/timings
./cleanupDB.sh ${AWSPROFILE} ${PREFIX}

#webapp
cd ${PREFIX}/tools/timings
deactivate
./webapp.sh ${AWSPROFILE} ${COUNT} ${PREFIX} ${REG} ${ACCT} ${FNI_TRIGGERBKT} #FNI_TRIGGERBKT without suffix
cd ${LOCALLIBDIR}  #this should be gammaRays directory for most 
source venv/bin/activate
cd ${PREFIX}/tools  #download the stream data and append it to stream base (save both)
python get_stream_data.py arn:aws:dynamodb:${REG}:${ACCT}:table/${SPOTTABLE}/stream/${SPOT_STREAMID} -p ${AWSPROFILE} >& streamS.new
diff -b -B streamS.base streamS.new| awk -F"> " '{print $2}' > webappS.stream
cat webappS.stream >> streamS.base
python get_stream_data.py arn:aws:dynamodb:${REG}:${ACCT}:table/${GAMMATABLE}/stream/${GR_STREAMID} -p ${AWSPROFILE} >&  streamD.new
diff -b -B streamD.base streamD.new| awk -F"> " '{print $2}' > webappD.stream
cat webappD.stream >> streamD.base
deactivate
./cleanupDB.sh ${AWSPROFILE} ${PREFIX}

#imageProc (and dbsync), capture streams for this one
cd ${PREFIX}/tools/timings
deactivate
./imageProc.sh ${AWSPROFILE} ${COUNT} ${PREFIX} ${REG} ${XREG} ${SPOTBKTWEST} ${IMG_DBSYNC_TRIGGER_TABLE_PREFIX} #IMG_DBSYNC_TRIGGER_TABLE_PREFIX without suffix
cd ${LOCALLIBDIR}  #this should be gammaRays directory for most 
source venv/bin/activate
cd ${PREFIX}/tools  #download the stream data and append it to stream base (save both)
python get_stream_data.py arn:aws:dynamodb:${REG}:${ACCT}:table/${SPOTTABLE}/stream/${SPOT_STREAMID} -p ${AWSPROFILE} >& streamS.new
diff -b -B streamS.base streamS.new| awk -F"> " '{print $2}' > imageProcS.stream
cat imageProcS.stream >> streamS.base
python get_stream_data.py arn:aws:dynamodb:${REG}:${ACCT}:table/${GAMMATABLE}/stream/${GR_STREAMID} -p ${AWSPROFILE} >&  streamD.new
diff -b -B streamD.base streamD.new| awk -F"> " '{print $2}' > imageProcD.stream
cat imageProcD.stream >> streamD.base
deactivate
./cleanupDB.sh ${AWSPROFILE} ${PREFIX}

cd ${PREFIX}/tools/timings
deactivate
#run synchronized map reduce job overhead measurement (second param is count, BDBENCH should be the name of your cloned big-data-benchmark folder)
export COUNT=25
export JOBID=job8000 #must match what is used in makeConfigs triggerBuckets datastructure for reducerCoordinator lambdas (currently job8000 which you can keep if you like)
nohup ./overheadMR.sh ${AWSPROFILE} ${COUNT} ${BDBENCH} ${PREFIX} ${JOBID} ${MR_TRIGGERBKTC} ${MR_TRIGGERBKTD} ${MR_TRIGGERBKTF} ${MR_TRIGGERBKTS} ${MR_TRIGGERBKTT} ${MR_TRIGGERBKTB}&
./cleanupDB.sh ${AWSPROFILE} ${PREFIX}

as an example:
./overheadMR.sh cjk1 5 cjk-gammaray-bdbenchmark /Users/ckrintz/RESEARCH/lambda/UCSBFaaS-Wrappers job8000 spot-mr-bkt-ns spot-mr-bkt-gr spot-mr-bkt-f pot-mr-bkt spot-mr-bkt-t spot-mr-bkt-b

############################## process data #############################
#summarize in the excel spreadsheet called result.xlxs in the paper repo
#commit all of the output files to the paper repo under results

#process timings data (cloudwatch) for micro
export COUNT=50 #if unset
cd ${LOCALLIBDIR}  #this should be gammaRays directory for most 
source venv/bin/activate
cd ${PREFIX}/tools
export MRDIR=${PREFIX}/gammaRay/apps/map-reduce
export CWDIR=${PREFIX}/tools/cloudwatch
python timings_parser.py ${MRDIR} ${CWDIR} out --micro_only --count ${COUNT} >micro.summary
#data is in out_lambdanameSuffix.out, e.g. out_s3writeS.out
deactivate

#process timings data (cloudwatch) for webapp and imageProc
export COUNT=50 #if unset
cd ${LOCALLIBDIR}  #this should be gammaRays directory for most 
source venv/bin/activate
cd ${PREFIX}/tools
export CWDIR=${PREFIX}/tools/cloudwatch
python apps_timings_parser.py ${CWDIR} out --count ${COUNT} > webapp_image_proc.summary
#data is in out_lambdaname_Suffix.out, e.g. out_ImageProcPy_S.out and out_SNSPy_S.out
deactivate

#timings_parser.py (processMicro function) needs to be extended to process mr results.


#process streams to generate pdfs for S (D is the same)
#this hasn't been tested for awhile and needs to be rewritten to get new format of data
#do not use until fixed
cd ${PREFIX}/tools
cd ${LOCALLIBDIR}  #this should be gammaRays directory for most 
source venv/bin/activate
python ddb_parser.py dbsyncS.stream
python ddb_parser.py imageProcS.stream
deactivate

#######  TROUBLESHOOTING ############
- if there are problems send Chandra the details and she can help until you get this all setup
- to see the logs after a run, remove the --delete from the downloadlogs.py calls and they will stay in cloudwatch for you to interrogate


#######  CLEANUP ############
#delete everything in AWS, and I mean everything! 
cd ${PREFIX}/tools/timings
#add any other lambdas you want to delete to restConfigsWest.json and restConfigsEast.json
#depending on where your lambdas are
./cleanupAWS.sh ${AWSPROFILE} ${REG} ${XREG}

#you can/should run cleanupDB.sh repeatedly until it says that the table is empty! this can take awhile
./cleanupDB.sh ${AWSPROFILE} ${PREFIX}    #you may have to temporarily increased provisioned writes if this hangs or takes a long time until you get to empty!
#if there are too many entries, the above will stop working. In this case just delete and remake the table:
aws dynamodb delete-table --table-name ${SPOTTABLE} --profile ${AWSPROFILE}
aws dynamodb delete-table --table-name ${GAMMATABLE} --profile ${AWSPROFILE}
#wait 5 mins then recreate them:
aws --profile ${AWSPROFILE} dynamodb create-table --region ${REG} --table-name ${SPOTTABLE} --attribute-definitions AttributeName=reqID,AttributeType=S --key-schema AttributeName=reqID,KeyType=HASH --provisioned-throughput ReadCapacityUnits=10,WriteCapacityUnits=20 --stream-specification StreamEnabled=true,StreamViewType=NEW_AND_OLD_IMAGES
#note the stream ID that comes back from this and use it to set this env var (if you miss it go to the dynamodb management console and get it under the Overview tab)
#just put the date/time component (after .../stream/) here for later use
export SPOT_STREAMID=2017-09-13T00:43:24.435  #set elsewhere in this file also!
aws --profile ${AWSPROFILE} dynamodb create-table --region ${REG} --table-name ${GAMMATABLE} --attribute-definitions AttributeName=reqID,AttributeType=S --key-schema AttributeName=reqID,KeyType=HASH --provisioned-throughput ReadCapacityUnits=10,WriteCapacityUnits=20 --stream-specification StreamEnabled=true,StreamViewType=NEW_AND_OLD_IMAGES
#note the stream ID that comes back from this and use it to set this env var (if you miss it go to the dynamodb management console and get it under the Overview tab)
#just put the date/time component (after .../stream/) here for later use
export GR_STREAMID=2017-09-13T00:39:21.716  #set elsewhere in this file also!

#delete all of the lambdas by name, in AWS (edit restConfigs[West,East].json to add others)
./cleanupLambdas.sh ${AWSPROFILE} ${AWSROLE} ${PREFIX}
#you now need to go and run makeConfigs.sh and all of the setupApps.py above
export END=100
for i in $(seq 1 $END); do rm -rf ${PREFIX}/gammaRay/apps/${i}; done
for i in $(seq 1 $END); do rm -rf ${PREFIX}/gammaRay/apps/map-reduce/${i}; done
for i in $(seq 1 $END); do rm -rf ${PREFIX}/tools/dynamodb/${i}; done
for i in $(seq 1 $END); do rm -rf ${PREFIX}/tools/cloudwatch/${i}; done
for i in $(seq 1 $END); do rm -rf ${PREFIX}/tools/cloudwatch/logs/; done

#download the streams, you will use these to diff out the old stuff when you get new stuff
#update these commands with your stream name
python ../get_stream_data.py arn:aws:dynamodb:${REG}:${ACCT}:table/${SPOTTABLE}/stream/${SPOT_STREAMID} -p ${AWSPROFILE} >& streamS.base
python ../get_stream_data.py arn:aws:dynamodb:${REG}:${ACCT}:table/${GAMMATABLE}/stream/${GR_STREAMID} -p ${AWSPROFILE} >&  streamD.base
#now you can start fresh
======================
other tools

#to download cloudwatch logs
deactivate
cd ${LOCALLIBDIR}
source venv/bin/activate
cd ${CWDIR}
#for log summaries since date time 1401861965497 
python downloadLogs.py /aws/lambda/FunctionName 1401861965497 -p ${AWSPROFILE} > log.out
#for log dump since date time 1401861965497 -- modify filter (boolean) at top of find_events in valid_event to add other lines based on strings in them
python downloadLogs.py /aws/lambda/FunctionName 1401861965497 -p ${AWSPROFILE} --noSummarize > log.out
#to delete all logs
python downloadLogs.py /aws/lambda/FunctionName 1401861965497 -p ${AWSPROFILE} --deleteOnly 
deactivate
======================
CASES:
C - nothing/clean
T - tracing
F - tracing + fleece daemon
S - static spotwrap (original)
D - dynamic spotwrap (gammaray)
B - fleece with dynamic spotwrap only for dependencies (gammaray) Publish, PutItem, PutObject
